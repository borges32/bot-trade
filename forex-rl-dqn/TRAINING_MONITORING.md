# ğŸ“Š Guia de Monitoramento do Treinamento PPO

## ğŸ¯ Objetivo das MudanÃ§as

O treinamento estava **piorando** porque:
- âŒ Learning rate muito baixo (0.0003) â†’ polÃ­tica estagnada
- âŒ Pouca exploraÃ§Ã£o (ent_coef=0.01) â†’ convergiu prematuramente
- âŒ Custos muito altos â†’ difÃ­cil aprender a lucrar
- âŒ Penalidade de risco alta â†’ agente muito conservador

## âœ… MudanÃ§as Aplicadas

| ParÃ¢metro | Antes | Depois | Motivo |
|-----------|-------|--------|--------|
| `learning_rate` | 0.0003 | **0.001** | Escapar de mÃ­nimo local |
| `ent_coef` | 0.01 | **0.03** | Mais exploraÃ§Ã£o |
| `commission` | 0.0002 | **0.0001** | Facilitar lucro inicial |
| `slippage` | 0.0001 | **0.00005** | Reduzir custos |
| `max_drawdown_pct` | 0.20 | **0.25** | EpisÃ³dios mais longos |
| `risk_penalty_lambda` | 0.1 | **0.05** | Menos conservador |

## ğŸš€ Como Reiniciar

### OpÃ§Ã£o 1: Parar atual e reiniciar do zero
```bash
# Para treinamento atual
pkill -f train_ppo.py

# Reinicia com novos parÃ¢metros
./restart_ppo_training.sh
```

### OpÃ§Ã£o 2: Deixar terminar e comeÃ§ar novo
```bash
# Aguarde o treinamento atual terminar (500k timesteps)
# Depois execute:
./restart_ppo_training.sh
```

### OpÃ§Ã£o 3: Treinar direto (recomendado)
```bash
# Para treinamento atual
pkill -f train_ppo.py

# Treina PPO do zero
python3 -m src.training.train_ppo --config config_hybrid.yaml
```

## ğŸ“ˆ MÃ©tricas Esperadas (Com Novos ParÃ¢metros)

### Primeiros 50k timesteps:
```
ep_rew_mean: -80 a -40  (melhor que -91)
clip_fraction: 0.05-0.15  (>0, significa que estÃ¡ aprendendo)
entropy_loss: -0.8 a -1.0  (explorando mais)
explained_variance: 0.3-0.5  (subindo)
value_loss: 0.5-0.7  (diminuindo)
```

### 100k-200k timesteps:
```
ep_rew_mean: -20 a +10  (comeÃ§ando a lucrar)
clip_fraction: 0.08-0.12
entropy_loss: -0.9 a -1.1
explained_variance: 0.5-0.7
value_loss: 0.3-0.5
```

### 300k-500k timesteps (final):
```
ep_rew_mean: +20 a +80  (lucrando consistentemente)
clip_fraction: 0.05-0.10
entropy_loss: -1.0 a -1.3
explained_variance: 0.7-0.9
value_loss: 0.2-0.4
```

## ğŸš¨ Sinais de Alerta

### âŒ Se apÃ³s 100k timesteps:
- `ep_rew_mean` ainda < -50 â†’ Aumentar `learning_rate` para 0.002
- `clip_fraction` = 0 â†’ Aumentar `learning_rate` ou `ent_coef`
- `explained_variance` < 0.3 â†’ Problema na value function

### âŒ Se apÃ³s 200k timesteps:
- `ep_rew_mean` < -20 â†’ Considere:
  - Reduzir mais `commission` (0.00005)
  - Aumentar `ent_coef` (0.05)
  - Aumentar `learning_rate` (0.002)

### âŒ Se `ep_rew_mean` > 0 mas instÃ¡vel:
- Reduzir `learning_rate` (0.0005)
- Reduzir `ent_coef` (0.02)

## ğŸ“Š Monitoramento em Tempo Real

### Ver mÃ©tricas a cada iteraÃ§Ã£o:
```bash
# Terminal 1: Executa treinamento
./restart_ppo_training.sh

# Terminal 2: Monitora logs
tail -f logs/hybrid/training.log
```

### Ver Ãºltimas 20 linhas de mÃ©tricas:
```bash
tail -20 logs/hybrid/training.log | grep -A 15 "rollout"
```

### Verificar progresso:
```bash
# Quantos timesteps jÃ¡ rodaram?
grep "total_timesteps" logs/hybrid/training.log | tail -1
```

## ğŸ¯ CritÃ©rios de Sucesso

### âœ… Treinamento BOM:
- `ep_rew_mean` **crescendo** ao longo do tempo
- `explained_variance` **> 0.6** no final
- `value_loss` **diminuindo**
- `clip_fraction` **> 0** (entre 5-15%)
- Final: `ep_rew_mean` **> 0** (lucrando)

### ğŸŸ¡ Treinamento MÃ‰DIO:
- `ep_rew_mean` **estÃ¡vel** mas negativo (-20 a 0)
- `explained_variance` 0.4-0.6
- Precisa de mais timesteps ou ajuste fino

### âŒ Treinamento RUIM:
- `ep_rew_mean` **piorando** (como estava antes)
- `clip_fraction` = 0 (estagnado)
- `value_loss` **aumentando**
- Precisa **reajustar hiperparÃ¢metros**

## ğŸ”§ Ajustes Finos (Se NecessÃ¡rio)

### Se nÃ£o melhorar apÃ³s mudanÃ§as:

**Edite `config_hybrid.yaml`:**

```yaml
ppo:
  params:
    learning_rate: 0.002  # Mais agressivo
    ent_coef: 0.05        # Ainda mais exploraÃ§Ã£o
    
  env:
    commission: 0.00005   # Custos mÃ­nimos
    reward_scaling: 2.0   # Recompensas maiores
```

**Depois:**
```bash
./restart_ppo_training.sh
```

## ğŸ“ Registro de Testes

Anote aqui os resultados para comparar:

### Teste 1 (Original - FALHOU):
- Config: lr=0.0003, ent=0.01, comm=0.0002
- Resultado (149k): ep_rew=-91.3, clip=0, var=0.416
- Status: âŒ Piorando

### Teste 2 (Ajustado - EM PROGRESSO):
- Config: lr=0.001, ent=0.03, comm=0.0001
- Resultado (50k): ___ (preencher)
- Resultado (100k): ___ (preencher)
- Resultado (200k): ___ (preencher)
- Status final: ___ (preencher)

### Teste 3 (Se necessÃ¡rio):
- Config: ___ (preencher)
- Resultado: ___ (preencher)
- Status: ___ (preencher)

## ğŸ’¡ Dicas

1. **PaciÃªncia**: PPO precisa de tempo para explorar
2. **Monitoramento**: Acompanhe a cada 50k timesteps
3. **Ajuste gradual**: Mude 1-2 parÃ¢metros por vez
4. **Baseline**: Sempre compare com run anterior
5. **Salvamento**: Modelos sÃ£o salvos a cada 50k timesteps

## ğŸ“ Entendendo os ParÃ¢metros

**`learning_rate`**: Velocidade de aprendizado
- Baixo (0.0001-0.0003): Lento, estÃ¡vel, pode estagnar
- MÃ©dio (0.0005-0.001): Balanceado
- Alto (0.002-0.005): RÃ¡pido, mas instÃ¡vel

**`ent_coef`**: ExploraÃ§Ã£o vs Exploitation
- Baixo (0.01): Exploita mais, pode convergir prematuramente
- MÃ©dio (0.03-0.05): Balanceado
- Alto (0.1+): Explora muito, pode nunca convergir

**`commission`**: Custo por trade
- Real (0.0002-0.0005): Realista para Forex
- Treino (0.0001): Facilita aprendizado inicial
- Depois ajustar para valor real

## âœ… Checklist Antes de Reiniciar

- [ ] Parou treinamento anterior: `pkill -f train_ppo.py`
- [ ] Conferiu mudanÃ§as em `config_hybrid.yaml`
- [ ] Terminal livre para rodar novo treinamento
- [ ] Pronto para monitorar por ~2-3 horas
- [ ] Tem espaÃ§o em disco (logs + checkpoints)

## ğŸš€ Comando de InÃ­cio

```bash
# Tudo pronto? Execute:
./restart_ppo_training.sh

# Ou manualmente:
python3 -m src.training.train_ppo --config config_hybrid.yaml
```

**Boa sorte! ğŸ€**
