# Sistema HÃ­brido de Trading: LightGBM + PPO

Sistema completo de trading para Forex baseado na combinaÃ§Ã£o de aprendizado supervisionado (LightGBM) e aprendizado por reforÃ§o (PPO).

## ğŸ¯ VisÃ£o Geral

Este sistema implementa uma arquitetura hÃ­brida onde:
- **LightGBM** aprende padrÃµes histÃ³ricos de preÃ§o e prevÃª direÃ§Ã£o/retorno futuro
- **PPO** aprende quando e como operar, usando sinais do LightGBM + contexto de mercado + gestÃ£o de risco

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Dados HistÃ³ricos (CSV)                    â”‚
â”‚                    (OHLCV do cTrader)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Feature Engineering                             â”‚
â”‚  (RSI, EMAs, MACD, Bollinger, ATR, Volatilidade, etc.)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚                             â”‚
          â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    LightGBM      â”‚          â”‚    Ambiente      â”‚
â”‚  (Supervisionado)â”‚          â”‚    Gym (PPO)     â”‚
â”‚                  â”‚          â”‚                  â”‚
â”‚ PrevÃª:           â”‚          â”‚ Estado:          â”‚
â”‚ â€¢ DireÃ§Ã£o        â”‚â”€â”€â”€â”€â”€â”€â”€â”€> â”‚ â€¢ Sinal LightGBM â”‚
â”‚ â€¢ Retorno        â”‚          â”‚ â€¢ Features       â”‚
â”‚   Futuro         â”‚          â”‚ â€¢ PosiÃ§Ã£o atual  â”‚
â”‚                  â”‚          â”‚ â€¢ PnL            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚ â€¢ Equity         â”‚
                              â”‚ â€¢ Drawdown       â”‚
                              â”‚                  â”‚
                              â”‚ AÃ§Ãµes:           â”‚
                              â”‚ 0 = Neutro       â”‚
                              â”‚ 1 = Comprar      â”‚
                              â”‚ 2 = Vender       â”‚
                              â”‚                  â”‚
                              â”‚ Reward:          â”‚
                              â”‚ PnL - custos     â”‚
                              â”‚ - penalizaÃ§Ã£o    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚   Agente PPO     â”‚
                              â”‚ (PolÃ­tica Neural)â”‚
                              â”‚                  â”‚
                              â”‚ Aprende:         â”‚
                              â”‚ â€¢ Timing         â”‚
                              â”‚ â€¢ GestÃ£o de riscoâ”‚
                              â”‚ â€¢ Maximizar PnL  â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                       â”‚
                                       â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚  API FastAPI     â”‚
                              â”‚                  â”‚
                              â”‚ /signal          â”‚
                              â”‚ /execute         â”‚
                              â”‚ /state           â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio
```bash
git clone <repo>
cd forex-rl-dqn
```

### 2. Crie ambiente virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Instale dependÃªncias
```bash
pip install -r requirements.txt
```

## ğŸ“Š PreparaÃ§Ã£o de Dados

### Formato do CSV (cTrader)
O sistema espera um CSV com as seguintes colunas:
```csv
timestamp,open,high,low,close,volume
2024-01-01 00:00:00,1.0950,1.0960,1.0945,1.0955,1000.0
2024-01-01 00:30:00,1.0955,1.0965,1.0950,1.0960,1200.0
...
```

Coloque seus arquivos CSV em `data/`:
- `data/usdjpy_history_30m.csv` (ou configure em `config_hybrid.yaml`)

## âš™ï¸ ConfiguraÃ§Ã£o

Edite `config_hybrid.yaml` para ajustar:

### Dados
```yaml
data:
  train_file: "data/usdjpy_history_30m.csv"
  val_split: 0.15
  test_split: 0.15
```

### LightGBM
```yaml
lightgbm:
  model_type: "classifier"  # ou "regressor"
  prediction_horizon: 5  # candles Ã  frente
  params:
    learning_rate: 0.05
    n_estimators: 500
    max_depth: 6
    # ... outros parÃ¢metros
```

### PPO
```yaml
ppo:
  env:
    initial_balance: 10000.0
    leverage: 1.0
    commission: 0.0002  # 0.02%
    stop_loss_pct: 0.02  # 2%
    take_profit_pct: 0.04  # 4%
  params:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    # ... outros parÃ¢metros
```

## ğŸš€ Uso

### 1. Treinar LightGBM
```bash
python -m src.training.train_lightgbm --config config_hybrid.yaml
```

Isso irÃ¡:
- Carregar dados histÃ³ricos
- Criar features tÃ©cnicas
- Treinar modelo LightGBM
- Salvar em `models/hybrid/lightgbm_model.txt`
- Exibir mÃ©tricas e feature importance

### 2. Treinar PPO
```bash
python -m src.training.train_ppo --config config_hybrid.yaml
```

Isso irÃ¡:
- Carregar LightGBM treinado
- Criar ambiente de trading
- Treinar agente PPO
- Salvar em `models/hybrid/ppo_model.zip`
- Exibir mÃ©tricas de performance

### 3. Subir API de InferÃªncia
```bash
cd src/inference
python service.py
```

A API estarÃ¡ disponÃ­vel em `http://localhost:8000`

#### Endpoints DisponÃ­veis:

**GET /** - InformaÃ§Ãµes da API
```bash
curl http://localhost:8000/
```

**GET /health** - Health check
```bash
curl http://localhost:8000/health
```

**POST /signal** - Obter sinal de trading
```bash
curl -X POST http://localhost:8000/signal \
  -H "Content-Type: application/json" \
  -d '{
    "candles": [
      {
        "timestamp": "2024-01-01T00:00:00",
        "open": 1.0950,
        "high": 1.0960,
        "low": 1.0945,
        "close": 1.0955,
        "volume": 1000.0
      },
      ... (mÃ­nimo 50 candles)
    ],
    "current_position": 0,
    "deterministic": true
  }'
```

Resposta:
```json
{
  "action": 1,
  "action_name": "comprar",
  "lightgbm_signal": 0.65,
  "confidence": 0.80,
  "current_state": {
    "position": 0,
    "balance": 10000.0,
    "equity": 10000.0,
    "unrealized_pnl": 0.0,
    "realized_pnl": 0.0,
    "total_return": 0.0,
    "max_drawdown": 0.0
  },
  "timestamp": "2024-01-01T12:00:00Z"
}
```

**POST /execute** - Executar aÃ§Ã£o
```bash
curl -X POST http://localhost:8000/execute \
  -H "Content-Type: application/json" \
  -d '{
    "action": 1,
    "price": 1.0955
  }'
```

**GET /state** - Obter estado atual
```bash
curl http://localhost:8000/state
```

**POST /reset** - Resetar estado
```bash
curl -X POST http://localhost:8000/reset
```

## ğŸ“ Exemplo de Uso Completo

```python
import requests
import pandas as pd

# 1. Carrega dados recentes (Ãºltimos 100 candles)
df = pd.read_csv('data/usdjpy_history_30m.csv').tail(100)

# 2. Converte para formato JSON
candles = df.to_dict('records')

# 3. Solicita sinal
response = requests.post(
    'http://localhost:8000/signal',
    json={
        'candles': candles,
        'current_position': 0,
        'deterministic': True
    }
)

signal = response.json()
print(f"AÃ§Ã£o: {signal['action_name']}")
print(f"ConfianÃ§a: {signal['confidence']:.2f}")

# 4. Se confianÃ§a > 0.6, executa
if signal['confidence'] > 0.6:
    execute_response = requests.post(
        'http://localhost:8000/execute',
        json={
            'action': signal['action'],
            'price': candles[-1]['close']
        }
    )
    print(f"Executado: {execute_response.json()}")
```

## ğŸ›ï¸ HiperparÃ¢metros Recomendados

### LightGBM (ClassificaÃ§Ã£o)
- `learning_rate`: 0.05
- `n_estimators`: 500
- `max_depth`: 6
- `num_leaves`: 31
- `prediction_horizon`: 5 candles

### PPO
- `learning_rate`: 0.0003
- `n_steps`: 2048
- `batch_size`: 64
- `gamma`: 0.99
- `total_timesteps`: 500000

### Ambiente
- `commission`: 0.0002 (0.02% - ajuste para seu broker)
- `slippage`: 0.0001 (0.01%)
- `stop_loss_pct`: 0.02 (2%)
- `take_profit_pct`: 0.04 (4%)

## ğŸ”§ Ajustes para Seu Contexto

### Par de Moedas
- Ajuste `commission` e `slippage` baseado no spread do seu par
- Pares mais volÃ¡teis podem precisar de `stop_loss_pct` maior

### Timeframe
- 5M: Use `prediction_horizon: 3-5`
- 15M: Use `prediction_horizon: 5-7`
- 30M: Use `prediction_horizon: 5-10`
- 1H: Use `prediction_horizon: 7-15`

### Custos de TransaÃ§Ã£o
Consulte seu broker e ajuste:
```yaml
ppo:
  env:
    commission: 0.0002  # Spread + comissÃ£o
    slippage: 0.0001    # Slippage mÃ©dio observado
```

### Alavancagem
```yaml
ppo:
  env:
    leverage: 1.0  # 1:1 (conservador)
    # leverage: 10.0  # 1:10 (agressivo - CUIDADO!)
```

## ğŸ“ˆ Monitoramento e AvaliaÃ§Ã£o

### MÃ©tricas do LightGBM
- **AUC** (classificaÃ§Ã£o): > 0.60 Ã© razoÃ¡vel, > 0.70 Ã© bom
- **Accuracy**: > 55% jÃ¡ adiciona valor
- **Direction Accuracy**: mais importante que RMSE

### MÃ©tricas do PPO
- **Mean Reward**: deve crescer durante treinamento
- **Mean Equity**: deve ser > initial_balance
- **Win Rate**: > 45% Ã© aceitÃ¡vel
- **Sharpe Ratio**: > 1.0 Ã© bom, > 2.0 Ã© excelente
- **Max Drawdown**: < 20% Ã© desejÃ¡vel

### Logs
Treinamento gera logs em:
- `logs/hybrid/train/` - Logs de treino PPO
- `logs/hybrid/val/` - Logs de validaÃ§Ã£o PPO

Visualize com TensorBoard:
```bash
tensorboard --logdir logs/hybrid
```

## ğŸ› Troubleshooting

### "LightGBM model not found"
Treine o LightGBM primeiro:
```bash
python -m src.training.train_lightgbm
```

### "Insufficient candles for reliable prediction"
Envie pelo menos 50 candles no request `/signal`

### Performance ruim
1. Aumente dados de treino (mÃ­nimo 6 meses de histÃ³rico)
2. Ajuste `prediction_horizon` para seu timeframe
3. Revise custos de transaÃ§Ã£o (commission/slippage)
4. Experimente diferentes features tÃ©cnicas
5. Treine por mais timesteps (PPO)

## ğŸ“š Estrutura do Projeto

```
forex-rl-dqn/
â”œâ”€â”€ config_hybrid.yaml           # ConfiguraÃ§Ã£o principal
â”œâ”€â”€ data/                        # Dados histÃ³ricos CSV
â”œâ”€â”€ models/                      # Modelos treinados
â”‚   â””â”€â”€ hybrid/
â”‚       â”œâ”€â”€ lightgbm_model.txt
â”‚       â”œâ”€â”€ ppo_model.zip
â”‚       â””â”€â”€ checkpoints/
â”œâ”€â”€ logs/                        # Logs de treinamento
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ common/                  # Utilities
â”‚   â”‚   â”œâ”€â”€ features.py         # Feature engineering
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ models/                  # Modelos
â”‚   â”‚   â”œâ”€â”€ lightgbm_model.py
â”‚   â”‚   â””â”€â”€ ppo_agent.py
â”‚   â”œâ”€â”€ envs/                    # Ambientes Gym
â”‚   â”‚   â””â”€â”€ forex_trading_env.py
â”‚   â”œâ”€â”€ training/                # Scripts de treino
â”‚   â”‚   â”œâ”€â”€ train_lightgbm.py
â”‚   â”‚   â””â”€â”€ train_ppo.py
â”‚   â””â”€â”€ inference/               # InferÃªncia e API
â”‚       â”œâ”€â”€ predictor.py
â”‚       â””â”€â”€ service.py           # API FastAPI
â””â”€â”€ requirements.txt
```

## ğŸ”¬ PrÃ³ximos Passos

1. **Backtesting Completo**: Implemente backtest walk-forward
2. **Multi-Timeframe**: Combine sinais de mÃºltiplos timeframes
3. **Ensemble**: Combine mÃºltiplos modelos LightGBM
4. **AÃ§Ã£o ContÃ­nua**: Experimente PPO com aÃ§Ã£o contÃ­nua (fraÃ§Ã£o do capital)
5. **Meta-Learning**: AdaptaÃ§Ã£o online aos novos dados

## ğŸ“„ LicenÃ§a

[Sua licenÃ§a aqui]

## ğŸ‘¥ ContribuiÃ§Ã£o

[InstruÃ§Ãµes de contribuiÃ§Ã£o]

---

**âš ï¸ AVISO IMPORTANTE**: Trading envolve risco significativo de perda. Este sistema Ã© para fins educacionais e de pesquisa. Sempre teste extensivamente em ambiente de simulaÃ§Ã£o antes de usar capital real. Nunca opere com dinheiro que nÃ£o pode perder.
